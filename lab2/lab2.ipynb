{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "319c83df",
   "metadata": {},
   "source": [
    "# AIC-5102B Lab 2 / Text classification\n",
    "\n",
    "The aim of this lab is to use LDA and its kernelized version to perform text classification. You should work by pairs, as I explained by email.\n",
    "\n",
    "## 0. Submission instructions\n",
    "\n",
    "Drop a ZIP archive of your working directory on https://mvproxy.esiee.fr by October 5th, 23:59. Late deliveries will be penalized 1pt/day. As you are working by pairs, any of you can submit for both; moreover, the last submission overwrites the former ones.\n",
    "\n",
    "Your working directory should contain:\n",
    "- This notebook (lab2.ipynb), filled with answers to the questions ;\n",
    "- and a *local* copy of the text articles or CSV files you are working with, which you should access to relatively and not absolutely. \n",
    "\n",
    "Please pay attention to the latter point. Code like\n",
    "```\n",
    "csv.reader('C:\\Users\\Yoyodyne\\My Documents\\AIC-5102B\\Lab2 Text Classification/dataset.csv')\n",
    "```\n",
    "\n",
    "should be banned, and replaced by\n",
    "```\n",
    "csv.reader('dataset.csv')\n",
    "```\n",
    "or\n",
    "```\n",
    "csv.read('./data/class1.csv')\n",
    "csv.read('./data/class2.csv')\n",
    "```\n",
    "\n",
    "I should be able to run your code on *Linux* without modifying the 'C:\\Users\\Yoyodyne\\My Documents\\AIC-5102B', which I don't have access to, nor changing anything else when I run your notebook. You must also stick to NLTK, and packages I include myself in the following code blocks, and nothing else.\n",
    "\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "\n",
    "You are free to choose the working dataset you wish to work with from Kaggle amongnst the following :\n",
    "- Ecommerce Text Classification\n",
    "- BBC Full Text Document Classification\n",
    "- Text Classification on Emails\n",
    "- Spam Text Message Classification\n",
    "\n",
    "See https://www.kaggle.com/datasets?search=text+classification\n",
    "\n",
    "Moreover, to alleviate the problem we will work with only 2 classes, so you will also have to choose 2 classes amongst those offered by your dataset.\n",
    "\n",
    "You should choose a dataset which contains documents which are long enough, otherwise it is unlikely that the specialized kernel of part 4 will help. Tweets, in particular, are discouraged for that reason. If you wish to use another dataset that those mentioned above, simply ask me for validation.\n",
    "\n",
    "Which dataset and classes did you chose ? Copy the related files to your working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ade3b1",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b79abd6",
   "metadata": {},
   "source": [
    "## 2. Text vectorization\n",
    "\n",
    "The following functions :\n",
    "- extract the vocabulary from row #5 (that number is arbitrary) of the specified CSV file \n",
    "- build the document-term matrix by reading again the same CSV file\n",
    "\n",
    "Adapt them, so that they fit your dataset and produce a D-T matrix in the end.\n",
    "\n",
    "Please note that:\n",
    "- the tokenization method used is wordpunct_tokenize(), which may not be optimal. You may call something different in case you find really too much garbage in your resulting vocabulary.\n",
    "- there are two \"if\" tests in dtmat_from_file which appear unnecessary so far. They are, indeed, because the test samples may include unseen words, which would generate out-of-bounds index. So unseen words are just ignored.\n",
    "- you may also want to lemmatize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "190415fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def voc_from_csv(csvfile):\n",
    "    nlines=0\n",
    "    voc=[]\n",
    "    with open(csvfile, errors='ignore') as file:\n",
    "        reader=csv.reader(file, delimiter=',')\n",
    "        for row in reader:\n",
    "            nlines=nlines+1\n",
    "            voc.extend(nltk.wordpunct_tokenize(row[1]))\n",
    "    voc=sorted(set(voc))\n",
    "    return voc,nlines\n",
    "\n",
    "def dtmat_from_csv(csvfile):\n",
    "    voc,rows=voc_from_csv(csvfile)\n",
    "    cols=len(voc)\n",
    "    mat=np.zeros((rows,cols))\n",
    "    d=0\n",
    "    with open(csvfile,  errors='ignore') as file:\n",
    "        reader=csv.reader(file, delimiter=',')\n",
    "        for row in reader:\n",
    "            w= nltk.wordpunct_tokenize(row[1])\n",
    "            X=np.searchsorted(voc,w,side='left')            \n",
    "            for i in range(0,len(w)):\n",
    "                if (X[i] < cols):\n",
    "                    if (w[i] == voc[X[i]]):\n",
    "                        mat[d][X[i]]+=1\n",
    "            d=d+1\n",
    "            print('Progress: ' + str(100*d/rows, ).split('.')[0] + '%', end='\\r')\n",
    "\n",
    "    return mat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4d9a6e",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "\n",
    "Run dtmat_from_csv on one of your sample dataset. Examine the resulting matrix. How many times does it happen that a given word is seen only once (possibly twice) in your training set ? Give a few lines which show this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd1b3754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100%\r"
     ]
    }
   ],
   "source": [
    "dt_mat = dtmat_from_csv('ecommerce.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6a219652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 80982\n",
      "Number of documents: 31133\n",
      "Words that apper only once: 16213\n",
      "Words that apper only twice: 17612\n",
      "Words that apper only once/twice: 33825\n"
     ]
    }
   ],
   "source": [
    "w_count = dt_mat.sum(axis=0)\n",
    "w_count = w_count.astype(int)\n",
    "\n",
    "print('Vocabulary size: ' + str(len(w_count)))\n",
    "print('Number of documents: ' + str(len(dt_mat)))\n",
    "once, twice = len(w_count[w_count == 1]), len(w_count[w_count == 2])\n",
    "print('Words that apper only once: ' + str(once))\n",
    "print('Words that apper only twice: ' + str(twice))\n",
    "print('Words that apper only once/twice: ' + str(once + twice))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b8d219",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Suppose we use ordinary LDA to separate positive from negative samples, computed by voc_from_csv(). What will be the effect of an increasing number of \"seen only once\" words :\n",
    "- on the memory complexity of the solution ?\n",
    "- on the solution itself ?\n",
    "\n",
    "Write your answer below, either in plain language or using LaTeX notation for formulas if you prefer. This question is purely theoretical and does not require any programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28fb3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62aa9d6f",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "No matter what your answer to question 2 is, add boolean parameter to voc_from_csv(), say \"purge\", so that when \"purge\" is True, the \"only seen once\" words are *not* included in the vocabulary. There is no need to modify dtmat_from_csv(),which has already been written to take this into account, except from the call to voc_from_csv().\n",
    "Change the block below accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1e8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtmat_from_csv(csvfile):\n",
    "    voc,rows=voc_from_csv(csvfile)\n",
    "    cols=len(voc)\n",
    "    mat=np.zeros((rows,cols))\n",
    "    d=0\n",
    "    with open(csvfile,  errors='ignore') as file:\n",
    "        reader=csv.reader(file, delimiter=',')\n",
    "        for row in reader:\n",
    "            w= nltk.wordpunct_tokenize(row[5])\n",
    "            X=np.searchsorted(voc,w,side='left')            \n",
    "            for i in range(0,len(w)):\n",
    "                if (X[i] < cols):\n",
    "                    if (w[i] == voc[X[i]]):\n",
    "                        mat[d][X[i]]+=1\n",
    "            d=d+1\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b10fb5",
   "metadata": {},
   "source": [
    "## 3. Ordinary LDA\n",
    "\n",
    "### Question 4\n",
    "\n",
    "Write an lda(X1,X2) function which computes and returns the direction predicted by the ordinary LDA given samples X1 and X2, which should be passed as matrices of row vectors. The function should return:\n",
    "- $\\boldsymbol{u}$, the optimal direction\n",
    "- and $s$, the abscissa of the hyperplane (the linear discriminant) on $\\boldsymbol{u}$ which optimally separates the data assuming they are normally distributed after their projection on $\\boldsymbol{u}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c107d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda(X1,X2):    \n",
    "# your code here\n",
    "    return u,s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2116dab",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Test your function on a toy sample. Count how many points are correcly classified. You may use/extend/modify the following piece of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac937407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X1=np.random.multivariate_normal([1,2], np.identity(2),30)\n",
    "X2=np.random.multivariate_normal([-3,0], 0.5*np.identity(2),20)\n",
    "plt.scatter(X1[:,0],X1[:,1])\n",
    "plt.scatter(X2[:,0],X2[:,1])\n",
    "plt.show()\n",
    "u,s= lda(X1,X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90684a7e",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Test the same function on your text classes. For at least one of them (most likeley, the lightest), you should normally have a problem. Why ? Add some code to your function to fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e472e60f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6dcc15f6",
   "metadata": {},
   "source": [
    "## 4. Kernelized LDA\n",
    "\n",
    "The kernelized version of LDA is implemented as a kfda package. Its homepage is here: https://pypi.org/project/kfda/\n",
    "You should install it by running pip3 install kfda\n",
    "\n",
    "### Question 7\n",
    "\n",
    "Let $\\boldsymbol{x}$ and $\\boldsymbol{y}$ be any two columns of your D-T matrix (which you may assume TD-IDF normalized or not, it does not change the problem). Consider the inhomogeneous polynomial kernel \n",
    "$$k(\\boldsymbol{x},\\boldsymbol{y})= (1+<\\boldsymbol{x},\\boldsymbol{y}>)^n$$\n",
    "where $n>0$ is integer.\n",
    "\n",
    "- Suppose that $n=2$, and that the above kernel is used in a kernelized LDA. What are the new axes created in the feature space, that didn't exist when $n=1$? Which of these could be useful, and change the solution computed by LDA in the feature space ?\n",
    "- Try to classify using this setup, and report your results. Then increase $n$ (moderately). What causes the computational bottleneck of kernelized LDA ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435382c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1fb3b61",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "We will now slightely improve the above kernel by replacing the natural dot product \n",
    "$$<\\boldsymbol{x},\\boldsymbol{y}>$$ \n",
    "by \n",
    "$$ \\sum_i \\min(\\boldsymbol{x}_i, \\boldsymbol{y}_i) $$\n",
    "resulting in\n",
    "\n",
    "$$f(\\boldsymbol{x},\\boldsymbol{y})= (1+\\sum_i \\min(\\boldsymbol{x}_i, \\boldsymbol{y}_i) )^n$$\n",
    "\n",
    "Is $f$ a positive semidefinite kernel ? Either prove that it is, or give a counter-example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2450b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45996a2d",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "Irrespective of your answer to question 8, try kfda with $f$ as its kernel. Looking at the source code https://github.com/concavegit/kfda/blob/master/kfda/kfda.py you will notice (line 92) that it relies on the paiwise_kernels function from sklearn to compute the Gram matrix. \n",
    "\n",
    "According to sklearn documentation https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_kernels.html the kernel parameter can be a callable, hence you can supply a function of your own as the kernel argument, possibly using the keywords field (kwds).\n",
    "\n",
    "Report your classification results, possibly varying $n$ (be reasonable with values, high $n$ may cause floating point exceptions, in addition of being meaningless). You should likely obtain decent (~ 75% accuracy, say) but not outstanding results.  This, however, is highly dependent on the dataset and classes you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99a4fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42178c26",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "One reason why the obtained accuracy is not fantastic is that the vector model we are using is blind to bigrams. For instance, we may encounter (normalized) words \"donald\", and \"trump\" separately in a document, but this is very different from \"donald trump\".\n",
    "\n",
    "One way to fix this is to include bigrams in the vocabulary : for two consecutive words, like \"donald trump\", we would add a synthetic word \"donald_trump\" to the vocabulary. \n",
    "\n",
    "What would be the memory complexity of the solution ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be14dd71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eec0fa64",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "Add an extra \"bigram\" parameter to voc_from_csv() to do so, and compare your results to those of question 9. Bigrams can be generated very simply using a code similar to this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fb98f57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I_think',\n",
       " 'think_traveling',\n",
       " 'traveling_to',\n",
       " 'to_Rio',\n",
       " 'Rio_next',\n",
       " 'next_winter',\n",
       " 'winter_would',\n",
       " 'would_be',\n",
       " 'be_great']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=['I','think','traveling','to','Rio','next','winter','would','be','great']\n",
    "[w[i]+'_'+w[i+1] for i in range(0,len(w)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b12aea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
